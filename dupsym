#!/usr/bin/env python3
"""
Duplicate File Symlinker with Reversible Operations
Finds duplicate files and replaces them with symlinks to save space.
"""

import argparse
import json
import os
from collections import defaultdict
from datetime import datetime
from pathlib import Path

import xxhash

BACKUP_FILE = '.symlink_backup.json'


def calculate_file_hash(filepath, chunk_size=8192):
    """Calculate xxhash of a file."""
    hasher = xxhash.xxh64()
    try:
        with open(filepath, 'rb') as f:
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        return hasher.hexdigest()
    except (IOError, OSError) as e:
        print(f'Error reading {filepath}: {e}')
        return None


def find_duplicates(directory='.'):
    """Find all duplicate files in the directory, excluding .git."""
    print(f'Scanning directory: {os.path.abspath(directory)}')

    # First pass: group by size
    size_map = defaultdict(list)
    file_count = 0

    for root, dirs, files in os.walk(directory):
        # Skip .git directory
        if '.git' in dirs:
            dirs.remove('.git')

        for filename in files:
            if filename.startswith('.'):
                continue

            filepath = os.path.join(root, filename)

            # Skip symlinks
            if os.path.islink(filepath):
                continue

            try:
                size = os.path.getsize(filepath)
                size_map[size].append(filepath)
                file_count += 1
            except OSError as e:
                print(f'Error accessing {filepath}: {e}')

    print(f'Found {file_count} files')

    # Second pass: check hash for files with same size
    hash_map = defaultdict(list)
    potential_duplicates = [
        files for files in size_map.values() if len(files) > 1
    ]

    print(
        f'Checking {sum(len(files) for files in potential_duplicates)} potential duplicates...'
    )

    for files in potential_duplicates:
        for filepath in files:
            file_hash = calculate_file_hash(filepath)
            if file_hash:
                hash_map[file_hash].append(filepath)

    # Filter to only actual duplicates
    duplicates = {h: files for h, files in hash_map.items() if len(files) > 1}

    return duplicates


def choose_keeper(files):
    """Choose which file to keep (shortest path, then alphabetically first)."""
    return min(files, key=lambda f: (len(f), f))


def create_symlinks(duplicates, dry_run=False):
    """Replace duplicate files with symlinks."""
    backup_data = {'timestamp': datetime.now().isoformat(), 'operations': []}

    total_saved = 0
    symlink_count = 0

    for file_hash, files in duplicates.items():
        keeper = choose_keeper(files)
        keeper_abs = os.path.abspath(keeper)

        print(f'\nDuplicate group (hash: {file_hash[:16]}...):')
        print(f'  Keeping: {keeper}')

        for duplicate in files:
            if duplicate == keeper:
                continue

            duplicate_abs = os.path.abspath(duplicate)
            file_size = os.path.getsize(duplicate)

            print(f'  Symlinking: {duplicate} -> {keeper_abs}')

            if not dry_run:
                # Backup original file info
                backup_data['operations'].append({
                    'symlink': duplicate_abs,
                    'target': keeper_abs,
                    'original_existed': True,
                    'size': file_size,
                })

                try:
                    # Remove the duplicate
                    os.remove(duplicate)
                    # Create symlink with absolute path
                    os.symlink(keeper_abs, duplicate_abs)
                    symlink_count += 1
                    total_saved += file_size
                except OSError as e:
                    print(f'  Error: {e}')
            else:
                symlink_count += 1
                total_saved += file_size

    if not dry_run and symlink_count > 0:
        # Save backup data
        with open(BACKUP_FILE, 'w') as f:
            json.dump(backup_data, f, indent=2)
        print(f'\nBackup data saved to {BACKUP_FILE}')

    print(f'\n{"[DRY RUN] " if dry_run else ""}Summary:')
    print(f'  Symlinks created: {symlink_count}')
    print(f'  Space saved: {total_saved / (1024 * 1024):.2f} MB')

    return symlink_count


def reverse_symlinks(backup_file=BACKUP_FILE):
    """Reverse the symlinking operation."""
    if not os.path.exists(backup_file):
        print(f'Error: Backup file {backup_file} not found!')
        return False

    with open(backup_file, 'r') as f:
        backup_data = json.load(f)

    print(f'Restoring from backup created at: {backup_data["timestamp"]}')
    print(f'Operations to reverse: {len(backup_data["operations"])}')

    restored_count = 0

    for op in backup_data['operations']:
        symlink_path = op['symlink']
        target_path = op['target']

        if not os.path.islink(symlink_path):
            print(f'Warning: {symlink_path} is not a symlink, skipping')
            continue

        if not os.path.exists(target_path):
            print(f'Error: Target file {target_path} no longer exists!')
            continue

        try:
            # Remove symlink
            os.remove(symlink_path)
            # Copy the original file back
            import shutil

            shutil.copy2(target_path, symlink_path)
            restored_count += 1
            print(f'Restored: {symlink_path}')
        except (OSError, IOError) as e:
            print(f'Error restoring {symlink_path}: {e}')

    print(f'\nRestored {restored_count} files')

    # Rename backup file
    backup_renamed = f'{backup_file}.restored.{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    os.rename(backup_file, backup_renamed)
    print(f'Backup file renamed to: {backup_renamed}')

    return True


def main():
    parser = argparse.ArgumentParser(
        description=
        'Find duplicate files and replace with symlinks (reversible)')
    parser.add_argument('directory',
                        nargs='?',
                        default='.',
                        help='Directory to scan (default: current directory)')
    parser.add_argument('--dry-run',
                        action='store_true',
                        help='Show what would be done without making changes')
    parser.add_argument('--reverse',
                        action='store_true',
                        help='Reverse previous symlinking operation')
    parser.add_argument('--backup-file',
                        default=BACKUP_FILE,
                        help=f'Backup file path (default: {BACKUP_FILE})')

    args = parser.parse_args()

    if args.reverse:
        reverse_symlinks(args.backup_file)
    else:
        duplicates = find_duplicates(args.directory)

        if not duplicates:
            print('\nNo duplicates found!')
            return

        print(f'\nFound {len(duplicates)} groups of duplicates')
        print(
            f'Total duplicate files: {sum(len(files) - 1 for files in duplicates.values())}'
        )

        if args.dry_run:
            print('\n[DRY RUN MODE - No changes will be made]')

        create_symlinks(duplicates, dry_run=args.dry_run)


if __name__ == '__main__':
    main()
