#!/data/data/com.termux/files/usr/bin/env python3
import argparse
import concurrent.futures as cf
import os
from collections import defaultdict
from pathlib import Path

import xxhash
from rich.progress import Progress

# ----------------------------------------
# GLOBAL STORAGE
# ----------------------------------------

SKIPPED_PATHS = []
EXCLUDED_DIRS = {'.git', '.venv', 'venv'}

# ----------------------------------------
# FILE HASHING (Worker Function)
# ----------------------------------------


def hash_file(path: str, chunk_size: int = 8192):
    """Worker function for concurrent futures.
    Computes xxhash64 hash of a file. Returns (path, hash).
    """
    path = Path(path)
    hasher = xxhash.xxh64()

    try:
        with open(path, 'rb') as f:
            for chunk in iter(lambda: f.read(chunk_size), b''):
                hasher.update(chunk)
    except (PermissionError, OSError):
        return path, None

    return path, hasher.hexdigest()


# ----------------------------------------
# FILE COLLECTION
# ----------------------------------------


def collect_all_files(directory: Path):
    """Collect files recursively, skipping excluded dirs."""
    files = []
    for root, dirs, fs in os.walk(directory, onerror=lambda e: None):
        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]
        for f in fs:
            files.append(Path(root) / f)
    return files


# ----------------------------------------
# STAGE 1: Prefilter by size
# ----------------------------------------


def group_by_size(files):
    groups = defaultdict(list)
    for f in files:
        try:
            size = f.stat().st_size
            groups[size].append(f)
        except (PermissionError, OSError):
            SKIPPED_PATHS.append(str(f))
    return groups


# ----------------------------------------
# STAGE 2: Concurrent Hashing
# ----------------------------------------


def hash_groups_in_parallel(groups):
    candidates = []
    for _size, paths in groups.items():
        if len(paths) > 1:
            candidates.extend(paths)

    if not candidates:
        return {}

    hash_groups = defaultdict(list)

    with Progress() as progress:
        task = progress.add_task('[cyan]Hashing files...',
                                 total=len(candidates))

        with cf.ProcessPoolExecutor() as executor:
            futures = {
                executor.submit(hash_file, str(p)): p
                for p in candidates
            }
            for future in cf.as_completed(futures):
                path, h = future.result()
                progress.update(task, advance=1)
                if h is None:
                    SKIPPED_PATHS.append(str(path))
                    continue
                hash_groups[h].append(str(path))

    return {h: ps for h, ps in hash_groups.items() if len(ps) > 1}


# ----------------------------------------
# DUPLICATE REMOVAL
# ----------------------------------------


def auto_delete_duplicates(dups) -> None:
    print('\nðŸ”¥ AUTO-DELETE MODE: Removing duplicates...\n')
    deleted_count = 0
    for _h, files in dups.items():
        duplicates = files[1:]
        for f in duplicates:
            try:
                os.remove(f)
                deleted_count += 1
            except Exception as e:
                print(f'âš ï¸ Could not delete {f}: {e}')
    print(f'\nâœ… Deleted {deleted_count} duplicate files.')


# ----------------------------------------
# REPORTING
# ----------------------------------------


def report_duplicates(dups):
    dup_count = sum(len(files) - 1 for files in dups.values())
    dup_size = sum(
        Path(f).stat().st_size for files in dups.values() for f in files[1:])
    print(f'\nðŸ“Š Report:')
    print(f'   â€¢ Duplicate groups: {len(dups)}')
    print(f'   â€¢ Total duplicate files: {dup_count}')
    print(f'   â€¢ Total duplicate size: {dup_size / 1024 / 1024:.2f} MB')


# ----------------------------------------
# MAIN
# ----------------------------------------


def main() -> None:
    parser = argparse.ArgumentParser(description='Ultra-Fast Duplicate Finder')
    parser.add_argument(
        '-a',
        '--auto-delete',
        action='store_true',
        help='Auto delete duplicates',
    )
    args = parser.parse_args()

    target = Path.cwd()  # Always current directory

    all_files = collect_all_files(target)
    size_groups = group_by_size(all_files)
    duplicates = hash_groups_in_parallel(size_groups)

    if duplicates:
        report_duplicates(duplicates)
        if args.auto_delete:
            auto_delete_duplicates(duplicates)
    else:
        print('\nâœ… No duplicates found.')


if __name__ == '__main__':
    main()
